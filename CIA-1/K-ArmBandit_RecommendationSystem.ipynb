{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9ead0a",
   "metadata": {},
   "source": [
    "### K-Arm Bandit based Solution for a Recommendation System using Epsilon Greedy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f4ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993df414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stimulating user feedback for each item\n",
    "def simulate_user_feedback(item):\n",
    "    true_reward_probs = [0.2, 0.5, 0.3, 0.6, 0.8, 0.4, 0.1, 0.7, 0.9, 0.2]  \n",
    "    return 1 if random.random() < true_reward_probs[item] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5cf8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon-greedy bandit algorithm for recommendation system\n",
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, k, epsilon, total_rounds):\n",
    "        self.k = k  # number of arms (items)\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.total_rounds = total_rounds  # total number of recommendations (users)\n",
    "        self.rewards = np.zeros(k)  # estimated rewards for each item\n",
    "        self.counts = np.zeros(k)  # number of times each item has been selected\n",
    "        self.total_reward = 0  # total reward across all recommendations\n",
    "        self.explorations = 0  # count of exploration actions\n",
    "        self.exploitations = 0  # count of exploitation actions\n",
    "\n",
    "   #selecting an item using epsilon-greedy method\n",
    "    def select_item(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: randomly select any item\n",
    "            self.explorations += 1\n",
    "            return np.random.randint(0, self.k)\n",
    "        else:\n",
    "            # Exploitation: select the item with the highest estimated reward\n",
    "            self.exploitations += 1\n",
    "            return np.argmax(self.rewards)\n",
    "        \n",
    "    #updating estimated rewards for the selected item based on the reward received\n",
    "    def update_reward(self, item, reward):\n",
    "        self.counts[item] += 1\n",
    "        #updating the average reward\n",
    "        self.rewards[item] += (reward - self.rewards[item]) / self.counts[item]\n",
    "        self.total_reward += reward\n",
    "\n",
    "    #running the recommendation systemfor thedefined number of rounds (uses)\n",
    "    def run(self):\n",
    "        for round in range(self.total_rounds):\n",
    "            #selecting an item to recommend\n",
    "            item = self.select_item()\n",
    "            \n",
    "            #simulating feedback from the user for the selected item\n",
    "            reward = simulate_user_feedback(item)\n",
    "            \n",
    "            #updating the reward estimates\n",
    "            self.update_reward(item, reward)\n",
    "        \n",
    "        print(f\"Results with epsilon = {self.epsilon}:\")\n",
    "        print(f\"Total Reward: {self.total_reward}\")\n",
    "        print(f\"Explorations: {self.explorations}, Exploitations: {self.exploitations}\")\n",
    "        print(f\"Final estimated rewards: {self.rewards}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7bf801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with epsilon = 0.3:\n",
      "Total Reward: 759\n",
      "Explorations: 299, Exploitations: 701\n",
      "Final estimated rewards: [0.21875    0.46875    0.38297872 0.71428571 0.79245283 0.45454545\n",
      " 0.11538462 0.67741935 0.89177489 0.15625   ]\n",
      "\n",
      "Results with epsilon = 0.1:\n",
      "Total Reward: 825\n",
      "Explorations: 86, Exploitations: 914\n",
      "Final estimated rewards: [0.18181818 0.61538462 0.125      0.54545455 0.84848485 0.58333333\n",
      " 0.33333333 0.66037736 0.8933162  0.375     ]\n",
      "\n",
      "Results with epsilon = 0.01:\n",
      "Total Reward: 828\n",
      "Explorations: 9, Exploitations: 991\n",
      "Final estimated rewards: [0.27819549 0.5        0.         0.         0.         0.5\n",
      " 0.         0.         0.91734575 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = 10  #number of items (arms)\n",
    "total_rounds = 1000  #number of rounds (recommendations)\n",
    "\n",
    "#usinf different values of epsilon for to observe trade-off between exploration & exploitation\n",
    "epsilons = [0.3, 0.1, 0.01]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    bandit = EpsilonGreedyBandit(K, epsilon, total_rounds)\n",
    "    bandit.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f6e43",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. High Exploration with ε = 0.3 - The system explore more often (30% of the time), recommending different items frequently. This leads to more diverse recommendations but lower rewards in total since few items can perform poorly.\n",
    "    \n",
    "2. Moderate Exploration with ε = 0.1 - Exploration happens less frequently here (10% of the time), leading to more exploitation of the known items. The rewards gained here are higher compared to ε=0.3.\n",
    "    \n",
    "3. Low Exploration with ε = 0.01 - The system rarely explores and always recommends the items that yields maximum rewards. The total reward is the highest because the system mostly exploits highly rewarding options. But, it might miss out in discovering better items due to limited exploration.\n",
    "    \n",
    "   \n",
    "Therefore, higher values of epsilon may result in more exploration and a diverse set of recommendations but may produce lower total rewards and lower values of epsilon may result in more exploration of the best rewarding items maximising short-term rewards but risking missing out on long-term opputunities for discovering highgly rewarding items. Setting a moderate epsilon value provides a good balance between exploration and exploitation leading to reasonably high rewards and diversity in recommendations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
